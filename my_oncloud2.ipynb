{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading dataset v1\n",
    "#dataset = pd.read_csv('combined_csv_v1.csv')\n",
    "dataset = pd.read_csv('combined_csv_v1.csv', nrows = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# view first 5 rows\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the datasets based on train-validation-test split of 70-15-15\n",
    "\n",
    "training_dataset = dataset.sample(frac=0.70, random_state=59)\n",
    "test_val_dataset = dataset.loc[~dataset.index.isin(training_dataset.index), :]\n",
    "\n",
    "testing_dataset = test_val_dataset.sample(frac=0.50, random_state=59)   \n",
    "validation_dataset = test_val_dataset.loc[~test_val_dataset.index.isin(testing_dataset.index), :]\n",
    "\n",
    "print(dataset.shape)\n",
    "print(training_dataset.shape)\n",
    "print(validation_dataset.shape)\n",
    "print(testing_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the train and test datasets to file\n",
    "\n",
    "training_dataset.to_csv('training_dataset.csv', index=False, header=False)\n",
    "validation_dataset.to_csv('validation_dataset.csv', index=False, header=False)\n",
    "testing_dataset.to_csv('testing_dataset.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing sagemaker \n",
    "\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'airplane-delays'\n",
    "training_data_path = sess.upload_data(path='training_dataset.csv', key_prefix=prefix + '/input/training')\n",
    "validation_data_path = sess.upload_data(path='validation_dataset.csv', key_prefix=prefix + '/input/validation')\n",
    "testing_data_path = sess.upload_data(path='testing_dataset.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "\n",
    "print(training_data_path)\n",
    "print(validation_data_path)\n",
    "print(testing_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing librarires and running linear regressor\n",
    "\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = get_image_uri(region, 'linear-learner')\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "\n",
    "ll_estimator = Estimator(container,\n",
    "    role=role, \n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix)\n",
    ")\n",
    "\n",
    "ll_estimator.set_hyperparameters(predictor_type='binary_classifier', \n",
    "                                 mini_batch_size=1000,\n",
    "                                epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the channels\n",
    "\n",
    "training_data_channel   = sagemaker.TrainingInput(s3_data=training_data_path, content_type='text/csv')\n",
    "validation_data_channel = sagemaker.TrainingInput(s3_data=validation_data_path, content_type='text/csv')\n",
    "\n",
    "ll_data = {'train': training_data_channel, 'validation': validation_data_channel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimator.fit(ll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset into S3 without the target column\n",
    "batch_test = testing_dataset.iloc[:,1:]\n",
    "batch_test.to_csv('batch-in.csv', index=False, header=False)\n",
    "batch_test_filepath = sess.upload_data(path='batch-in.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "print(batch_test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/batch-out/'.format(bucket, prefix)\n",
    "print(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a batch transform on the test data\n",
    "ll_transformer = ll_estimator.transformer(instance_count=1,\n",
    "                                            instance_type='ml.c5.9xlarge',\n",
    "                                            strategy='MultiRecord',\n",
    "                                            assemble_with='Line',\n",
    "                                            output_path=batch_output)\n",
    "\n",
    "ll_transformer.transform(data=batch_test_filepath,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "\n",
    "ll_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the results from S3\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()), names=['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted_real = [int(label[-1]) for label in target_predicted.index]\n",
    "target_predicted_real[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_labels = testing_dataset.iloc[:,0]\n",
    "matrix = confusion_matrix(test_labels, target_predicted_real)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(matrix, square=True, annot=True, fmt='d', cbar=False, cmap='mako', linewidths=0.5,\n",
    "                xticklabels=['Not Delayed', 'Delayed'],\n",
    "                yticklabels=['Not Delayed', 'Delayed'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"\\nTest Accuracy\\n\", metrics.accuracy_score(test_labels, target_predicted_real))\n",
    "print(\"\\nRecall\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nPrecision\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[1,0]))\n",
    "print(\"\\nSensitivity\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nSpecificity\\n\", (matrix[1,1])/(matrix[1,0]+ matrix[1,1]))\n",
    "print(\"\\nF1 Score\\n\", (matrix[0,0])/(matrix[0,0]+ 0.5*(matrix[1,0]+matrix[0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, target_predicted_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading dataset v1\n",
    "#dataset = pd.read_csv('combined_csv_v2.csv')\n",
    "dataset = pd.read_csv('combined_csv_v2.csv', nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# view first 5 rows\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the datasets based on train-validation-test split of 70-15-15\n",
    "\n",
    "training_dataset = dataset.sample(frac=0.70, random_state=59)\n",
    "test_val_dataset = dataset.loc[~dataset.index.isin(training_dataset.index), :]\n",
    "\n",
    "testing_dataset = test_val_dataset.sample(frac=0.50, random_state=59)   \n",
    "validation_dataset = test_val_dataset.loc[~test_val_dataset.index.isin(testing_dataset.index), :]\n",
    "\n",
    "\n",
    "print(dataset.shape)\n",
    "print(training_dataset.shape)\n",
    "print(validation_dataset.shape)\n",
    "print(testing_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the train and test datasets to file\n",
    "\n",
    "training_dataset.to_csv('training_dataset.csv', index=False, header=False)\n",
    "validation_dataset.to_csv('validation_dataset.csv', index=False, header=False)\n",
    "testing_dataset.to_csv('testing_dataset.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sagemaker \n",
    "\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'airplane-delays'\n",
    "training_data_path = sess.upload_data(path='training_dataset.csv', key_prefix=prefix + '/input/training')\n",
    "validation_data_path = sess.upload_data(path='validation_dataset.csv', key_prefix=prefix + '/input/validation')\n",
    "testing_data_path = sess.upload_data(path='testing_dataset.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "\n",
    "print(training_data_path)\n",
    "print(validation_data_path)\n",
    "print(testing_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing librarires and running linear regressor\n",
    "\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = get_image_uri(region, 'linear-learner')\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "\n",
    "ll_estimator = Estimator(container,\n",
    "    role=role, \n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix)\n",
    ")\n",
    "\n",
    "ll_estimator.set_hyperparameters(predictor_type='binary_classifier', \n",
    "                                 mini_batch_size=1000,\n",
    "                                epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the channels\n",
    "\n",
    "training_data_channel   = sagemaker.TrainingInput(s3_data=training_data_path, content_type='text/csv')\n",
    "validation_data_channel = sagemaker.TrainingInput(s3_data=validation_data_path, content_type='text/csv')\n",
    "\n",
    "ll_data = {'train': training_data_channel, 'validation': validation_data_channel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimator.fit(ll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset into S3 without the target column\n",
    "batch_test = testing_dataset.iloc[:,1:]\n",
    "batch_test.to_csv('batch-in.csv', index=False, header=False)\n",
    "batch_test_filepath = sess.upload_data(path='batch-in.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "print(batch_test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/batch-out/'.format(bucket, prefix)\n",
    "print(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a batch transform on the test data\n",
    "ll_transformer = ll_estimator.transformer(instance_count=1,\n",
    "                                            instance_type='ml.c5.9xlarge',\n",
    "                                            strategy='MultiRecord',\n",
    "                                            assemble_with='Line',\n",
    "                                            output_path=batch_output)\n",
    "\n",
    "ll_transformer.transform(data=batch_test_filepath,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "\n",
    "ll_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the results from S3\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()), names=['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted_real = [int(label[-1]) for label in target_predicted.index]\n",
    "target_predicted_real[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_labels = testing_dataset.iloc[:,0]\n",
    "matrix = confusion_matrix(test_labels, target_predicted_real)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(matrix, square=True, annot=True, fmt='d', cbar=False, cmap='mako', linewidths=0.5,\n",
    "                xticklabels=['Not Delayed', 'Delayed'],\n",
    "                yticklabels=['Not Delayed', 'Delayed'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"\\nTest Accuracy\\n\", metrics.accuracy_score(test_labels, target_predicted_real))\n",
    "print(\"\\nRecall\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nPrecision\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[1,0]))\n",
    "print(\"\\nSensitivity\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nSpecificity\\n\", (matrix[1,1])/(matrix[1,0]+ matrix[1,1]))\n",
    "print(\"\\nF1 Score\\n\", (matrix[0,0])/(matrix[0,0]+ 0.5*(matrix[1,0]+matrix[0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, target_predicted_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading dataset v1\n",
    "# dataset = pd.read_csv('combined_csv_v1.csv')\n",
    "dataset = pd.read_csv('combined_csv_v1.csv', nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# view first 5 rows\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the datasets based on train-validation-test split of 70-15-15\n",
    "\n",
    "training_dataset = dataset.sample(frac=0.70, random_state=59)\n",
    "test_val_dataset = dataset.loc[~dataset.index.isin(training_dataset.index), :]\n",
    "\n",
    "testing_dataset = test_val_dataset.sample(frac=0.50, random_state=59)   \n",
    "validation_dataset = test_val_dataset.loc[~test_val_dataset.index.isin(testing_dataset.index), :]\n",
    "\n",
    "print(dataset.shape)\n",
    "print(training_dataset.shape)\n",
    "print(validation_dataset.shape)\n",
    "print(testing_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the train and test datasets to file\n",
    "\n",
    "training_dataset.to_csv('training_dataset.csv', index=False, header=False)\n",
    "validation_dataset.to_csv('validation_dataset.csv', index=False, header=False)\n",
    "testing_dataset.to_csv('testing_dataset.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sagemaker \n",
    "\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'airplane-delays'\n",
    "training_data_path = sess.upload_data(path='training_dataset.csv', key_prefix=prefix + '/input/training')\n",
    "validation_data_path = sess.upload_data(path='validation_dataset.csv', key_prefix=prefix + '/input/validation')\n",
    "testing_data_path = sess.upload_data(path='testing_dataset.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "\n",
    "print(training_data_path)\n",
    "print(validation_data_path)\n",
    "print(testing_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = get_image_uri(region, 'xgboost', repo_version='1.0-1')\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "\n",
    "xgb_estimator = Estimator(container,\n",
    "    role=role, \n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix)\n",
    ")\n",
    "\n",
    "xgb_estimator.set_hyperparameters(objective='multi:softmax',\n",
    "                                  num_class='2',\n",
    "                                  num_round=10,\n",
    "                                  early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_channel   = sagemaker.TrainingInput(s3_data=training_data_path, content_type='text/csv')\n",
    "validation_data_channel = sagemaker.TrainingInput(s3_data=validation_data_path, content_type='text/csv')\n",
    "\n",
    "xgb_data = {'train': training_data_channel, 'validation': validation_data_channel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator.fit(xgb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset into S3 without the target column\n",
    "batch_test = testing_dataset.iloc[:,1:]\n",
    "batch_test.to_csv('batch-in.csv', index=False, header=False)\n",
    "batch_test_filepath = sess.upload_data(path='batch-in.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "print(batch_test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/batch-out/'.format(bucket, prefix)\n",
    "print(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a batch transform on the test data\n",
    "xgb_transformer = xgb_estimator.transformer(instance_count=1,\n",
    "                                            instance_type='ml.c5.9xlarge',\n",
    "                                            strategy='MultiRecord',\n",
    "                                            assemble_with='Line',\n",
    "                                            output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_test_filepath,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the results from S3\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()), names=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_labels = testing_dataset.iloc[:,0]\n",
    "matrix = confusion_matrix(test_labels, target_predicted)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(matrix, square=True, annot=True, fmt='d', cbar=False, cmap='mako', linewidths=0.5,\n",
    "                xticklabels=['Not Delayed', 'Delayed'],\n",
    "                yticklabels=['Not Delayed', 'Delayed'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"\\nTest Accuracy\\n\", metrics.accuracy_score(test_labels, target_predicted))\n",
    "print(\"\\nRecall\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nPrecision\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[1,0]))\n",
    "print(\"\\nSensitivity\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nSpecificity\\n\", (matrix[1,1])/(matrix[1,0]+ matrix[1,1]))\n",
    "print(\"\\nF1 Score\\n\", (matrix[0,0])/(matrix[0,0]+ 0.5*(matrix[1,0]+matrix[0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAtaset2 -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading dataset v1\n",
    "#dataset = pd.read_csv('combined_csv_v2.csv')\n",
    "dataset = pd.read_csv('combined_csv_v2.csv', nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# view first 5 rows\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the datasets based on train-validation-test split of 70-15-15\n",
    "\n",
    "training_dataset = dataset.sample(frac=0.70, random_state=59)\n",
    "test_val_dataset = dataset.loc[~dataset.index.isin(training_dataset.index), :]\n",
    "\n",
    "testing_dataset = test_val_dataset.sample(frac=0.50, random_state=59)   \n",
    "validation_dataset = test_val_dataset.loc[~test_val_dataset.index.isin(testing_dataset.index), :]\n",
    "\n",
    "print(dataset.shape)\n",
    "print(training_dataset.shape)\n",
    "print(validation_dataset.shape)\n",
    "print(testing_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the train and test datasets to file\n",
    "\n",
    "training_dataset.to_csv('training_dataset.csv', index=False, header=False)\n",
    "validation_dataset.to_csv('validation_dataset.csv', index=False, header=False)\n",
    "testing_dataset.to_csv('testing_dataset.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sagemaker \n",
    "\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'airplane-delays'\n",
    "training_data_path = sess.upload_data(path='training_dataset.csv', key_prefix=prefix + '/input/training')\n",
    "validation_data_path = sess.upload_data(path='validation_dataset.csv', key_prefix=prefix + '/input/validation')\n",
    "testing_data_path = sess.upload_data(path='testing_dataset.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "\n",
    "print(training_data_path)\n",
    "print(validation_data_path)\n",
    "print(testing_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = get_image_uri(region, 'xgboost', repo_version='1.0-1')\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "\n",
    "xgb_estimator = Estimator(container,\n",
    "    role=role, \n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix)\n",
    ")\n",
    "\n",
    "xgb_estimator.set_hyperparameters(objective='multi:softmax',\n",
    "                                  num_class='2',\n",
    "                                  num_round=10,\n",
    "                                  early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_channel   = sagemaker.TrainingInput(s3_data=training_data_path, content_type='text/csv')\n",
    "validation_data_channel = sagemaker.TrainingInput(s3_data=validation_data_path, content_type='text/csv')\n",
    "\n",
    "xgb_data = {'train': training_data_channel, 'validation': validation_data_channel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator.fit(xgb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset into S3 without the target column\n",
    "batch_test = testing_dataset.iloc[:,1:]\n",
    "batch_test.to_csv('batch-in.csv', index=False, header=False)\n",
    "batch_test_filepath = sess.upload_data(path='batch-in.csv', key_prefix=prefix + '/input/testing')\n",
    "\n",
    "print(batch_test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/batch-out/'.format(bucket, prefix)\n",
    "print(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a batch transform on the test data\n",
    "xgb_transformer = xgb_estimator.transformer(instance_count=1,\n",
    "                                            instance_type='ml.c5.9xlarge',\n",
    "                                            strategy='MultiRecord',\n",
    "                                            assemble_with='Line',\n",
    "                                            output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_test_filepath,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the results from S3\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()), names=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_labels = testing_dataset.iloc[:,0]\n",
    "matrix = confusion_matrix(test_labels, target_predicted)\n",
    "matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(matrix, square=True, annot=True, fmt='d', cbar=False, cmap='mako', linewidths=0.5,\n",
    "                xticklabels=['Not Delayed', 'Delayed'],\n",
    "                yticklabels=['Not Delayed', 'Delayed'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"\\nTest Accuracy\\n\", metrics.accuracy_score(test_labels, target_predicted))\n",
    "print(\"\\nRecall\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nPrecision\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[1,0]))\n",
    "print(\"\\nSensitivity\\n\", (matrix[0,0])/(matrix[0,0]+ matrix[0,1]))\n",
    "print(\"\\nSpecificity\\n\", (matrix[1,1])/(matrix[1,0]+ matrix[1,1]))\n",
    "print(\"\\nF1 Score\\n\", (matrix[0,0])/(matrix[0,0]+ 0.5*(matrix[1,0]+matrix[0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COnclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
